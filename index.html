<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<link rel="shortcut icon" href="./pic/cuhk.ico">
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<meta name="keywords" content="Jie Yang">
	<meta name="description" content="Jie Yang's home page">
	<meta name="google-site-verification" content="xg04VgCH_1GseFDVDJUDOU2fWPkMU61koxePfxI9L2U" />
	<link rel="stylesheet" href="jemdoc.css" type="text/css">
	<title>Jie Yang&#39;s Homepage</title>
</head>
<body>
	<!-- <nav class="navbar navbar-dark navbar-expand-lg fixed-top">
		<div id="layout-menu">
			<a href="#publication">Publication</a>
			<a href="#experience">Experience</a>
			<a href="#service">Service</a>
			<a href="#presentation">Presentation</a>
			<a href="#award">Award</a>
			<a href="#patent">Patent</a>
			<a href="#teaching">Teaching</a>
		</div>
	</nav> -->
<div id="layout-content" style="margin-top:25px">
	
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1></h1>
					<h1>Jie Yang &nbsp;&nbsp;<font face="Arial">杨杰</font></h1>
				</div>
				<h3>PhD Student</h3>
				<p>
					Computer and Information Engineering<br>
					The Chinese University of Hong Kong, Shenzhen<br>
					<br>
					Email: <a href="mailto:jieyang5@link.cuhk.edu.cn">jieyang5@link.cuhk.edu.cn</a><br>
				</p>
				<p>
					<a href="https://scholar.google.com/citations?user=UVzG9IcAAAAJ&hl=zh-CN"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/yangjie-cv"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
				</p>
			</td>
			<td>
				<img src="./pic/yangjie.jpg" border="0" width="200" style="margin-right: 50px;"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>
<h2>Biography</h2>
<p>
	I am a PhD student since 2021.9, at The Chinese University of Hong Kong, Shenzhen</a>, 
	co-supervised by <a href="http://www.zhangruimao.site/">Prof. Ruimao Zhang</a> and <a href="https://mypage.cuhk.edu.cn/academics/lizhen/">Prof. Zhen Li</a>.
</p>
<p> My research focus is on open-world perception and multimodal understanding/generation. My long-term research goal is to develop intelligent machines that can actively perceive, analyze, and interpret human states, behaviors, and underlying motivations in dynamic scenes. </p>
<h2>News</h2>
<ul>
								<li>
		One paper is accepted to T-PAMI.
	</li>
							<li>
		One paper is accepted to CVPR2025.
	</li>
						<li>
		One paper is accepted to ICRA2025.
	</li>
					<li>
		One paper is accepted to NeurIPS2024.
	</li>
							<li>
		<a href="https://arxiv.org/abs/2303.05499"> Grounding DINO</a> is selected as <a href="https://www.paperdigest.org/2024/09/most-influential-eccv-papers-2024-09/"> The Most Influential Paper</a> in ECCV 2024.

						
	</li>
				<li>
		Three papers are accepted to ECCV2024.
	</li>
			<li>
		One paper is accepted to CVPR2024.
	</li>
			<li>
		We present <a href="https://arxiv.org/pdf/2310.08530.pdf"> X-Pose</a> to detect any keypoints of any objects.
	</li>
	<li>
	<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"> Grounded SAM</a> is accepted to ICCV 2023 Demo Track.
		</li>
	<li>
		One paper is accepted to ICCV2023.
	</li>
	<li>
		Two papers are accepted to MIDL2023 and one is rated as an oral presentation.
	</li>
	<li>
		One paper is accepted to CVPR2023.
	</li>
	<li>
		One paper is accepted to ICLR2023.
	</li>
	<li>
		One paper is accepted to NeurIPS2022.
	</li>
</ul>
		<h2> Selected Publications [<a href="https://scholar.google.com/citations?user=UVzG9IcAAAAJ&hl=zh-CN">Google Scholar</a>]</h2>

			<li><a href="">ED-Pose++: Enhanced Explicit Box Detection for Conventional and Interactive Multi-Object Keypoint Detection
</a><br>
<b>Jie Yang</b>, Ailing Zeng, Tianhe Ren, Shilong Liu, Feng Li, Ruimao Zhang, Lei Zhang.<br>
IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2025.
	</li>
	
		<li><a href="">InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing
</a><br>
Jinlu Zhang, Yixin Chen, Zan Wang, <b>Jie Yang</b>, Yizhou Wang, Siyuan Huang.<br>
IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.
	</li>
	
		<li><a href="">Unlock the Power of Unlabeled Data in Language Driving Model
</a><br>
Chaoqun Wang, <b>Jie Yang</b>, Xiaobin Hong, Ruimao Zhang.<br>
IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2025
	</li>
	
		<li><a href="https://arxiv.org/pdf/2411.01846">KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension
</a><br>
<b>Jie Yang</b>, Wang Zeng, Sheng Jin, Lumin Xu, Wentao Liu, Chen Qian, Ruimao Zhang.<br>
 Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024
	</li>
	
		<li><a href="https://arxiv.org/pdf/2310.08530.pdf">X-Pose: Detecting Any Keypoints
</a><br>
<b>Jie Yang</b>, Ailing Zeng, Ruimao Zhang, Lei Zhang.<br>
 European Conference on Computer Vision (<b>ECCV</b>), 2024
	</li>
			<li><a href="https://arxiv.org/pdf/2407.12435">F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions
</a><br>
<b>Jie Yang</b>, Xuesong Niu, Nan Jiang, Ruimao Zhang, Siyuan Huang.<br>
 European Conference on Computer Vision (<b>ECCV</b>), 2024
	</li>
				<li><a href="https://arxiv.org/pdf/2406.07221">Open-World Human-Object Interaction Detection via Multi-modal Prompts
</a><br>
<b>Jie Yang</b>, Bingliang Li, Ailing Zeng, Lei Zhang, Ruimao Zhang.<br>
IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.
  <br />
</li>
	
	<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Neural_Interactive_Keypoint_Detection_ICCV_2023_paper.pdf">Neural Interactive Keypoint Detection
</a><br>
<b>Jie Yang</b>, Ailing Zeng, Feng Li, Shilong Liu, Ruimao Zhang, Lei Zhang.<br>
IEEE International Conference on Computer Vision (<b>ICCV</b>), 2023.
	</li>
<li> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Semantic_Human_Parsing_via_Scalable_Semantic_Transfer_Over_Multiple_Label_CVPR_2023_paper.pdf">Semantic Human Parsing via Scalable Semantic Transfer over Multiple Label Domains
</a><br>
<b>Jie Yang</b>, Chaoqun Wang, Zhen Li, Junle Wang, Ruimao Zhang.<br>
IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.
</li>	
				<li><a href="https://arxiv.org/pdf/2302.01593.pdf">Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation
</a><br>
<b>Jie Yang</b>, Ailing Zeng, Shilong Liu, Feng Li, Ruimao Zhang, Lei Zhang.<br>
International Conference on Learning Representations (<b>ICLR</b>), 2023.
</li>	
				
<li><a href="https://arxiv.org/pdf/2206.10571.pdf">Toward Unpaired Multi-modal Medical Image Segmentation via Learning Structured Semantic Consistency
</a><br>
<b>Jie Yang</b>, Ye Zhu, Chaoqun Wang, Zhen Li, Ruimao Zhang.<br>
International Conference on Medical Imaging with Deep Learning (<b>MIDL</b>), 2023.
</li>
</ul>				
	<h2>Honors &amp; Awards</h2>
		<ul>
    <li><p> The First Prize Scholarship, 2018，2019，2020. </p></li>
    <li><p> National Scholarship, 2018 </p></li>
		</ul>
		<div id="footer">
			<div id="footer-text"></div>
		</div>
		<p>
			<center>
				<div id="clustrmaps-widget" style="width:40%">
					<script type='text/javascript' id='clustrmaps'
						src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=yqzHjRzsBJQtkz3lH1jgeuQAOnTiZWl3UFnWM5FeXZM&cmo=ff0000&cmn=ffb800&ct=ffffff'></script>
				</div>
				<br>&copy; Jie Yang | Last updated: Sep 2023
			</center>
		</p>
	</div>
</body>
</html>
</body>
</html>
